## ðŸ‘‹ About me
- I am currently a postdoctoral researcher at the [Gaoling School of Artificial Intelligence](https://ai.ruc.edu.cn/), [Renmin University of China](https://www.ruc.edu.cn/). I earned my PhD from [University of Montreal](https://www.umontreal.ca/en/), where I was mentored by Prof. [Jian-Yun Nie](http://rali.iro.umontreal.ca/nie/jian-yun-nie-en/).
- I completed my master's (2019) and bachelor's (2016) degrees at Renmin University of China, under the guidance of Prof. [Zhicheng Dou](http://playbigdata.ruc.edu.cn/dou) and Prof. [Ji-Rong Wen](https://scholar.google.com/citations?user=tbxCHJgAAAAJ), delving into various NLP challenges.
- Research interests: Retrieval-augmented generation, large language models for information retrieval, session-based document ranking

## ðŸ“Ž Homepages
- Personal Pages: [https://daod.github.io/](https://daod.github.io/)
- Google Scholar: [https://scholar.google.com/citations?user=tBqVOWsAAAAJ](https://scholar.google.com/citations?user=tBqVOWsAAAAJ)
- DBLP: [https://dblp.org/pid/71/9704-1.html](https://dblp.org/pid/71/9704-1.html)

## ðŸ”¥ News
- *2024.1*: We propose a new instruction tuning dataset (INTERS) for unlocking the power of LLMs on search tasks. See more [details](https://arxiv.org/abs/2401.06532).
- *2023.11*: We analyze the risk of data leakage in LLM pre-training and write a new paper to alert this problem. See more [details](https://arxiv.org/abs/2311.01964).
- *2023.8*: We write a new survey about applying large language models for information retrieval. See more [details](https://arxiv.org/abs/2308.07107).
- *2023.8*: We publish a new version of YuLan-Chat. It achieves better performance than the official LLaMA-2 and LLaMA-2-Chat on MMLU, C-Eval, and AGI-Gaokao benchmarks! <a href="https://github.com/RUC-GSAI/YuLan-Chat"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/RUC-GSAI/YuLan-Chat"></a> 
